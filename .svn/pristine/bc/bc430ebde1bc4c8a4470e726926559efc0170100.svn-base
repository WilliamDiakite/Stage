import os

import scipy.stats as stats
import matplotlib.pyplot as plt

import numpy as np
from pandas import read_csv
from sklearn.preprocessing import MinMaxScaler


def random_split(dataX, dataY):
	'''
		Selects random samples from 
		normal and anomalous datasets

		Return :
			dataX : shuffled normal set
			dataY : shuffled anomalous set
	'''
	n_sampleX = int(dataX.shape[0]*0.5)
	n_sampleY = int(dataY.shape[0]*0.5)

	print('Number of normal samples :', n_sampleX)
	print('Number of anomalous samples :', n_sampleY)

	np.random.shuffle(dataX)
	np.random.shuffle(dataY)


	actual = np.hstack((np.zeros(dataX.shape[0]), np.ones(dataY.shape[0])))

	return np.vstack((dataX, dataY)), actual


def get_datasets(path_normal, path_anomalous):

	# load the dataset
	normal_dataset = read_csv(path_normal, delimiter=',')
	anomalous_dataset = read_csv(path_anomalous, delimiter=',')

	labels = list(normal_dataset)
	print(labels)
	time = normal_dataset.values[:,0]

	print(time.shape)

	normal_dataset = normal_dataset.values[:, 1:]
	anomalous_dataset = anomalous_dataset.values[:, 1:]

	# split into train and test sets
	train_size = int(normal_dataset.shape[0] * 0.60)
	test_size = normal_dataset.shape[0] - int(train_size/2)
	val_size = normal_dataset.shape[0] - test_size
	train_data, test_data_normal = normal_dataset[0:train_size,:], normal_dataset[train_size:train_size+val_size,:]
	validation = normal_dataset[train_size+val_size:,:]


	# normalize the normal_dataset (with train scaler)
	scaler = MinMaxScaler(feature_range=(0, 1))

	train_data = scaler.fit_transform(train_data)
	test_data_normal = scaler.transform(test_data_normal)
	validation = scaler.transform(validation)
	test_data_anomalous = scaler.transform(anomalous_dataset)

	# Export normalized train_data to csv
	norm = np.vstack((time[:train_data.shape[0]], train_data.T)) 
	filename = 'norm_' + os.path.basename(path_normal)
	np.savetxt(filename, norm.T[:300,:], header=','.join(labels), delimiter=',')

	return train_data, validation, test_data_normal, test_data_anomalous
		


def get_dataset_files(dataset_path):
	'''
		returns file lists
	'''

	# Get paths to data
	path_normal		= dataset_path + '/normal/normalized/'
	path_anomalous	= dataset_path + '/anomalous/normalized/'

	# Compute sizes of normal sets
	nb_files = len(os.listdir(path_normal))
	train_nb = int(nb_files * 0.60)
	test_nb  = nb_files - train_nb

	# Shuffle file list for cross validation
	l_file = os.listdir(path_normal)
	np.random.shuffle(np.asarray(l_file))

	train_files = [path_normal+x for x in l_file[:train_nb]]
	test_files_n= [path_normal+x for x in l_file[train_nb:]]
	test_files_a= [path_anomalous+x for x in os.listdir(path_anomalous)]

	return train_files, test_files_n, test_files_a



def drange(start, stop, increment):
    while start < stop:
        yield start
        start += increment



def plot(fp_rate, vp_rate, filename):

	line_x = [0,1]

	plt.plot(fp_rate, vp_rate, color="blue", linewidth=1.0, linestyle="-")
	plt.plot(line_x, line_x, color="red", linewidth=1.0, linestyle="--")

	plt.xlabel('False positive rate')
	plt.ylabel('True positive rate')
	plt.axis([0, 1, 0, 1])

	plt.savefig(filename)






class DatasetInfo(object):
	'''
		Collect information from a dataset folder
		Doesn't store any data
	'''

	def __init__(self, path_to_dataset):

		self.path = path_to_dataset


	def nb_features(self):

		path_normal = self.path + '/normal/normalized/'

		for filename in os.listdir(path_normal):
			file = path_normal + filename
			tmp = read_csv(file)
			return tmp.values.shape[1]

	def nb_normal_sample(self):
		path_normal = self.path + '/normal/normalized/'
		sum_sample = 0

		for filename in os.listdir(path_normal):
			file = path_normal + filename
			tmp = read_csv(file)

			sum_sample += tmp.values.shape[0]

		return sum_sample

	def nb_normal_files(self):
		path_normal = self.path + '/normal/normalized/'
		return len(os.listdir(path_normal))

	def nb_anomalous_sample(self):
		path_normal = self.path + '/anomalous/normalized/'
		sum_sample = 0

		for filename in os.listdir(path_anomalous):
			file = path_normal + filename
			tmp = read_csv(file)

			sum_sample += tmp.values.shape[0]

		return sum_sample

	def nb_anomalous_files(self):
		path_normal = self.path + '/anomalous/normalized/'
		return len(os.listdir(path_anomalous))

