import numpy as np
import utils 
from utils import DatasetInfo, drange
from class_autoencoder import Autoencoder
import tensorflow as tf
from pandas import read_csv
from sklearn.preprocessing import MinMaxScaler



'''
Activation functions :
	tf.nn.relu(features, name=None)
	tf.nn.relu6(features, name=None)
	tf.nn.crelu(features, name=None)
	tf.nn.elu(features, name=None)
	tf.nn.softplus(features, name=None)
	tf.nn.softsign(features, name=None)
	tf.sigmoid(x, name=None)	(default)
	tf.tanh(x, name=None)


Optimizers :
	tf.train.Optimizer
	tf.train.GradientDescentOptimizer  (default)
	tf.train.AdadeltaOptimizer
	tf.train.AdagradOptimizer
	tf.train.AdagradDAOptimizer
	tf.train.MomentumOptimizer
	tf.train.AdamOptimizer     
	tf.train.FtrlOptimizer
	tf.train.ProximalGradientDescentOptimizer
	tf.train.ProximalAdagradOptimizer
	tf.train.RMSPropOptimizer
'''


#-------------------------------------------------------------------------------#
# 							APPLICATION'S PARAMETERS							#
#-------------------------------------------------------------------------------#

#--- DATA PATH ---#

# Path to "normal" dataset
dataset_path 	= './../../../Ressources/Generated_files/Datasets/normalization_test/'

# Stored models
stored_models = './Saved_models/'


#--- MODEL PARAMETERS ---#

# Model architecture (hidden layers only)
#hidden_layers = [10, 4, 10]

hidden_layers = [30,  12, 30]
#hidden_layers 	= [100, 64, 45, 32, 45, 64, 100]

# Learning parameters
activation		= tf.nn.sigmoid
learning_rate 	= 0.1cd Datasets
optimizer		= tf.train.AdamOptimizer(learning_rate)
batch_size		= 20
epochs 			= 1
file_start 		= 0.0
file_end   		= 1


#--- Threshold parameters ---#

# plage de recherche du seuil
max_threshold = 300000

# pas de la recherche du seuil
threshold_step	= 30000



#-------------------------------------------------------------------------------#
# 							TRAINING AND SCORING 								#
#-------------------------------------------------------------------------------#


if __name__ == '__main__':

	from argparse import ArgumentParser
	parser = ArgumentParser()
	parser.add_argument('-reset', action='store_true', help="If set, model weights will be reinitialized")
	parser.add_argument('-force_reset', action='store_true', help="Reset model (if created) without asking for permission")
	parser.add_argument("mode", type=str, help='Choose application mode',
						choices=['train_score', 'train', 'score'])
	parser.add_argument('name', type=str, help='Name of the model')
	args = parser.parse_args()


	# Fix random seed for reproducibility
	np.random.seed(7)

	# Initialize a data information collector
	di = DatasetInfo(dataset_path)

	train_files, test_files_n, test_files_a = utils.get_dataset_files(dataset_path)


	# Initialize model
	model = Autoencoder(model_name=args.name, 
						input_dim=di.nb_features(), 
						hidden_layers=hidden_layers, 
						optimizer=optimizer)

	if args.force_reset is False:

		# Restore model parameters if needed
		if args.reset_model is True:
			while True:
				choice_ar = '[ ! ] Are you sure you want to reset model "' + args.name +'" ? [y/n]'
				choice = input(choice_ar)

				if choice == 'y' or choice == 'Y':
					print('[ ! ] "', args.name, '" will not be restored')
					break

				elif choice == 'n' or choice == 'N':
					
					model_path = stored_models + args.name + '.ckpt'
					
					try:		
						model.restore(model_path)
						print('[ + ] Model "', args.name, '" has been restored')
					except:
						print('[ ! ] Model "', args.name, '" not founded, creating model...')
					break

		elif args.reset_model is False:
			model_path = stored_models + args.name + '.ckpt'
			
			try:
				model.restore(model_path)
				print('[ + ] Model "', args.name, '" is now loaded')
			except:
				print(model_path)
				print('[ + ] Model"', args.name, '" is not created yet, creating model...')

	elif args.force_reset is True:
		print('[ ! ] Model "', args.name, '" has been forced to reset')



	# Train the model and compute score
	if args.mode == 'train_score':
		
		# Training model
		print('[ + ] Training and testing model')
		model.train_files(train_files=train_files, 
						training_epochs=epochs,
						batch_size=batch_size, 
						start_file_prct=file_start, 
						end_file_prct = file_end)

		# Compute roc curve
		model.plot_roc(max_threshold, threshold_step, test_files_n, test_files_a)
				

	# Only train the model
	elif args.mode == 'train':

		print('[ + ] Training and testing model')
		model.train_files(train_files=train_files, 
						training_epochs=epochs,
						batch_size=batch_size, 
						start_file_prct=file_start, 
						end_file_prct = file_end)

	# Compute score only
	elif args.mode == 'score':
		model.plot_roc(max_threshold, threshold_step, test_files_n, test_files_a)



	print('[DONE]')